<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="author" content="Script Tutorials" />
    <meta name="description" content="Responsive Websites Using BootStrap - demo page">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Prof. K. Sreenivasa Rao | Professor | CSE | IIT Kharagpur </title>

    <!-- css stylesheets -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</head>
<body>

    <!-- modal box -->
    <div class="modal fade" id="my-modal-box" tabindex="-1" role="dialog" aria-labelledby="my-modal-box-l" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="modal-header">
            <button type="button" class="close" data-dismiss="modal" aria-hidden="true">&times;</button>
            <div class="modal-title" id="my-modal-box-l">
              <h3>Share it</h3>
            </div>
          </div><!-- /.modal-header -->
          <div class="modal-body">
            <p>Share it box content</p>
            <!-- AddThis Button BEGIN -->
            <div class="addthis_toolbox addthis_default_style addthis_32x32_style">
            <a class="addthis_button_preferred_1"></a>
            <a class="addthis_button_preferred_2"></a>
            <a class="addthis_button_preferred_3"></a>
            <a class="addthis_button_preferred_4"></a>
            <a class="addthis_button_compact"></a>
            <a class="addthis_counter addthis_bubble_style"></a>
            </div>
            <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-4dbfb1915f17d240"></script>
          </div><!-- /.modal-body -->
        </div><!-- /.modal-content -->
      </div><!-- /.modal-dialog -->
    </div><!-- /.modal -->

    <!-- fixed navigation bar -->
    <div class="navbar navbar-fixed-top navbar-inverse" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#b-menu-1">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">Prof. K.Sreenivasa Rao</a>
        </div>
        <div class="collapse navbar-collapse" id="b-menu-1">
          <ul class="nav navbar-nav navbar-right">
            <li><a href="bio.html">Biography</a></li>
            <li><a href="res.html">Research</a></li>
            <li><a href="cou.html">Courses</a></li>
            <li><a href="pub.html">Publications</a></li>
            <li><a href="pro.html">Projects</a></li>
            <li class="active"><a href="stu.html">Students</a></li>
            
              </ul>
            </li>
          </ul>
        </div> <!-- /.nav-collapse -->
      </div> <!-- /.container -->
    </div> <!-- /.navbar -->

    <!-- slider -->
    
    

    <!-- main container -->
    <div class="container">

      <!-- second menu bar -->
      <nav class="navbar navbar-default navbar-static">
        <div class="navbar-header">
            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#b-menu-2">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#"></a>
        </div>

        <!-- submenu elements for #b-menu-2 -->
        <div class="collapse navbar-collapse" id="b-menu-2">
    

        </div><!-- /.nav-collapse -->
      </nav>

      <!-- 2-column layout -->
      <div class="row row-offcanvas row-offcanvas-right">
        <div class="col-xs-12 ">

          <!-- jumbotron -->
          <div class="jumbotron">
            <h1>Students - Alumni (PhD / MS)</h1>
          </div>

          <div class="row">
          
          
          <div class="container">
  <h2>PhD </h2>
   <!-- 4 PhD students -->
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/shashi.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Shashidhar G. Koolagudi </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Emotion Recognition from Speech using Source, System, and Prosodic Features
	   <br><strong>Present Affiliation :</strong> Associate Professor, NIT Surathkal
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal1"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal1" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Shashidhar G. Koolagudi</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Excitation source features derived from LP residual, instants of significant excitation, and glottal pulse signal are proposed to recognize the emotions.</li>
					<li>	Spectral features such as LPCCs and MFCCs extracted from consonant, vowel, and CV (consonant-vowel) transition regions are proposed for emotion recognition.</li>
					<li>	Pitch synchronously extracted spectral features and formant related features are also investigated for characterizing and classifying the emotions.</li>
					<li>	Local and global prosodic features extracted from sentence, word, and syllable levels are proposed to derive multiple evidence to strengthen the emotion classification performance.</li>
					<li>	Fusion techniques are proposed to combine the evidence obtained from source, system and prosodic features to enhance the emotion recognition performance.</li>
					<li>	A two-stage emotion recognition approach based on speaking rate, has been proposed for improving the recognition performance.</li>
					<li>	Different pattern classifiers such as GMM, SVM, and AANNs are used to develop models to recognize emotions.</li>
					<li>	The simulated emotion speech corpus in Telugu is developed to promote the research on emotion speech processing in the context of Indian languages. The database contains 12000 utterances simulated by 10 speakers (5 male and 5 female) in 8 different emotions. The proposed features and models are evaluated using the developed corpus.</li>
					<li>	The proposed features and models are also evaluated using the natural emotions collected from Hindi movies.</li>
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   

    </div>
  </div>
  <hr>
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/anil.JPG" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Anil Kumar Vuppala </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Vowel onset point detection for speech processing in mobile environment
	    <br><strong>Present Affiliation :</strong> Assistant Professor, IIIT Hyderabad
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal2"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal2" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Anil Kumar Vuppala</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Methods are proposed for the detection of VOPs in presence of speech coding and background noise conditions.</li>
					<li>	A two-stage hybrid approach based on Hidden Markov Models (HMMs) and Support Vector Machines (SVMs) is proposed for improving the performance of Consonant-Vowel (CV) recognition system.</li>
					<li>	Two-stage VOP detection method is proposed for spotting CV units from continuous speech.</li>
					<li>	Combined temporal and spectral preprocessing methods are explored to improve the performance of CV recognition system under background noise.</li>
					<li>	A method based on VOPs is proposed to improve the performance of Speaker Identification (SI) system in presence of coding.</li>
					<li>	A method is proposed for non-uniform time scale modification using VOPs and instants of significant excitation.</li>
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
    </div>
  </div>
  <hr>
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/avatar_mr.png" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Nirmalya Sen </h4>
      <p><strong>Area of Research :</strong> Speaker Recognition
       <br><strong>Thesis Title :</strong> Enhancement of Speaker Recognition Performance for Short Test Segments using GMM-SVM and Polynomial Classifiers  
	   <br><strong>Present Affiliation :</strong> Research Engineer (Audio Analytics), Videonetics Technology Private Limited
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal3"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal3" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Nirmalya Sen</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Thesis reveals that, for long test segments the GMM-SVM classifier performs better than the classical GMM-UBM classifier. However, for short test segments the performance of the GMM-SVM classifier is very poor compared to the classical GMM-UBM classifier. We have shown that, the poor performance of GMM-SVM classifier for short test segments is due to the difference between the amount of MAP adaptations of training and test GMM supervectors.</li>
					<li>	To overcome the problem of poor performance of GMM-SVM classifier in recognition of short test segments, we have proposed the partitioning of training utterance (UP). Results revealed that, the partitioning of training utterance could improve the accuracy of GMM-SVM classifier significantly for short test segments. It was clear from the results that with proper utterance partitioning, the accuracy of GMM-SVM classifier for short test segments also was significantly better than the classical GMM-UBM classifier. For long test segments also, the overall performance of GMM-SVM classifier improved slightly due to the partitioning of training utterance.</li>
					<li>	We have shown that, as the dimension of the GMM supervector increases (i.e., the model order of the UBM increases), the average between-class-distance decreases. Results also revealed that, as the dimension of the GMM supervector increases the more number of support vectors are required for SVM training. Hence, for GMM-SVM classifier, average number of cohorts per speaker increases when the dimension of the GMM supervector increases. We have given a clear intuitive explanation for greater overlap of various classes in the higher dimensional GMM supervector space.</li>
					<li>	We have thoroughly investigated the necessity of utterance partitioning for speakers belonging to the impostor class in GMM-SVM classifier in recognition of short test segments. We have shown that, if we do not perform utterance partitioning for speakers of impostor class then, the average number of cohorts per speaker increases significantly and for short test segments, the performance of GMM-SVM classifier degraded drastically. It is also observed that, as the dimension of the GMM supervector increases, the necessity of partitioning of training utterances for the speakers belonging to the impostor class also increases.</li>
					<li>	We have investigated the effects of data imbalance problem in GMM-SVM classifier with minority (positive) class over-sampling approach. We have shown that, performance of GMM-SVM classifier has not improved even after sufficient enhancement of the number of support vectors for the positive class of the SVM classifier. Thesis discusses a clear intuitive explanation for, why the problem of data imbalance between speaker class and impostor class does not have any adverse consequence on the performance of GMM-SVM classifier.</li>
					<li>	We have also investigated the effects of data imbalance problem in GMM-SVM classifier using majority class (i.e., negative class) under-sampling approach (NCUS) by removing the support vectors of the majority class. Thesis demonstrates that, after NCUS operation, the decision hyperplane of the SVM classifier shifts toward the ideal position; however, the orientation of the decision hyperplane is not satisfactory. Results confirm that, NCUS operation improves the performance of the GMM-SVM classifier when false alarm probability is high. However, when false alarm probability is low, the performance of the GMM-SVM classifier decreases after NCUS operation due to the alteration of cohorts (i.e., rejection of proper cohorts).</li>
					<li>	We have also performed experiments using max rule speaker comparison with inner product discriminant functions (MRSCIPDF) technique which is the extreme case of majority class under-sampling approach. Thesis reveals that, similar to the GMM-SVM classifier, the partitioning of training utterance operation can improve the performance of MRSCIPDF technique significantly for recognition of short test segments and modestly for long test segments.</li>
					<li>	Thesis comprehensively concludes that, the improvement in speaker verification performance after partitioning of the training utterances in GMM-SVM classifier is not due to the reduction in data imbalance problem. Reduction in the amount of mismatch of MAP adaptations between training GMM supervectors and test GMM supervectors is the reason for improvement in the performance of GMM-SVM classifier when we apply partitioning of training utterance.</li>
					<li>	We have shown that, data imbalance problem has adverse consequence on the performance of the polynomial classifier. The speaker identification performance of the polynomial classifier trained with mean-square error (MSE) training criterion is much poorer compared to the performance of the GMM classifier.</li>
					<li>	To overcome the problem of data imbalance in a polynomial classifier, we have proposed the weighted mean-square error (WMSE) training criterion for the polynomial classifier. The speaker identification performance of the polynomial classifier trained with weighted mean-square error (WMSE) training criterion is far better compared to the performance of the polynomial classifier trained with mean-square error (MSE) training criterion. With proper selection of weight, the speaker identification performance of the polynomial classifier trained with WMSE training criterion is better than the performance of the GMM classifier. </li>
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
    </div>
  </div>
  <hr>
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/narendra.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Narendra N P </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Source Modeling for Improvement of Quality of HMM-based Speech Synthesis
	     <br><strong>Present Affiliation :</strong> Postdoctoral Researcher, Aalto University (Finland)
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal4"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal4" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Narendra N P</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	A robust voicing detection and F 0 estimation method is proposed based on the ZFF method.</li>
					<li>	A parametric source modeling method is proposed based on the principal component analysis of pitch-synchronous residual frames of the excitation signal.</li>
					<li>	Based on the analysis of characteristics of the residual frames around GCI, a parametric source modeling method is proposed which models the residual frames as a combination of deterministic and noise components.</li>
					<li>	A hybrid source modeling method is proposed by utilizing the optimal residual frames extracted from the excitation signal of a phone.</li>
					<li>	A hybrid source modeling method is proposed based on the time-domain deterministic plus noise model.</li>
					<li>	Automatic detection of creaky voice method is proposed based on the variance of the epoch parameters. The epoch parameters which characterize the source of excitation are analyzed in modal and creaky regions.</li>
					<li>	The hybrid source model is proposed for generating the creaky voice at appropriate places in the synthesized speech.</li>				
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
    </div>
  </div>
  <br>
  <hr>
  
   <div class="media" >
    <div class="media-left media-top">
      <img src="images/jainath.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Jainath Yadav </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Emotion Transformation using Significant Events of Speech
	     <br><strong>Present Affiliation :</strong> Assistant Professor, Central University of South Bihar
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModl4"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModl4" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Jainath Yadav </h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	A method based on zero time windowing is proposed for epoch detection from emotional speech.</li>
					<li>	Methods are proposed for the detection of vowel onset and offset points from neutral and emotional speech.</li>
					<li>	A method for detecting vowel transition regions has been proposed based on epoch locations and rate of change of first formant.</li>
					<li>	A non-uniform emotion-specific prosody incorporation method is proposed using significant events of speech.</li>
					<li>	Gaussian Mixture Models (GMMs) and Feedforward Neural Networks (FFNNs) based mapping functions have been developed for transforming prosodic and spectral features from neutral to desired emotions.</li>
					<li>	Emotional speech synthesis systems have been developed by incorporating the emotion-specific knowledge derived from manual rule-base and automatic mapping functions.</li>
					<li>	Emotion transformation framework has been explored for developing the robust Speaker Identification (SI) and Consonant-Vowel (CV) recognition systems in the emotional environments.</li>				
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
    </div>
  </div>
  <br>
  <hr>
  
  
  
  
  
  
  
  
  <h2>MS </h2>
  
  <!-- 12 MS students -->
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/ramu.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Ramu Reddy Vempada </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Prosody Modeling for Syllable based Text-to-Speech Synthesis
	      <br><strong>Present Affiliation :</strong> Researcher, TCS Innovation Labs
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal5"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal5" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Ramu Reddy Vempada</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	A baseline unit selection based Bengali text-to-speech synthesis system is developed using syllable as the basic unit. Several issues such as text corpus collection, optimal text selection, deriving letter to sound (LTS) rules, recording and labeling the speech corpus, proper prosody models are addressed as part of developing the baseline TTS.</li>
					<li>	Duration modeling using feedforward neural networks (FFNN) is proposed to predict the appropriate durations of the sequence of syllables. Positional, contextual, phonological and articulatory features are proposed in modeling the durations.</li>
					<li>	Intonation modeling using two-stage feedforward neural networks is proposed to predict the fundamental frequencies (F 0 ’s) at start, middle and end positions of syllables. Three F 0 ’s per syllable are used to capture the broad shape of the intonation contour. Tilt parameters in addition to positional, contextual and phonological features are proposed to model the intonation of the sequence of syllables.</li>
					<li>	Intensity modeling using feedforward neural networks (FFNN) is proposed to predict the appropriate intensities of the sequence of syllables. Positional, contextual and phonological features are proposed in modeling the intensities.</li>
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
    </div>
  </div>
  <hr>
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/narendra.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Narendra N P </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Optimal Unit Selection method for Syllable based Text-to-Speech Synthesis in Bengali
	         <br><strong>Present Affiliation :</strong> Postdoctoral Researcher, Aalto University (Finland)
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal6"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal6" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Narendra N P</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	A baseline syllable based text-to-speech synthesis system is developed in Bengali language. Several issues such as text corpus collection, optimal text selection, deriving letter to sound rules, recording and labeling the speech corpus, are addressed which are essential in building the baseline TTS.</li>
					<li>	Concatenation costs are defined based on type of syllable joins. Natural measure of continuity is proposed for each type of syllable joins and accordingly concatenation costs are proposed to reduce the perceptual distortions at the syllable boundaries.</li>
					<li>	Three-stage target cost formulation is proposed for selecting appropriate units from the database.</li>
					<li>	Genetic algorithm based method is proposed for tuning the weights of unit selection cost function.</li>
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
    </div>
  </div>
  <hr>
 
 <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/krishnendu.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Krishnendu Ghosh </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Text Analysis for Bengali Text-to-Speech Synthesis Sytem
	   <br><strong>Present Affiliation :</strong> PhD Scholar, IIT Kharagpur
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal7"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal7" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Krishnendu Ghosh</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	A baseline text analysis block is developed for syllable-based Bengali text-to-speech synthesis systems. Text and speech corpora are collected for building the text analysis modules. The baseline text analysis block is developed with minimal text normalization and minimal grapheme-to-phoneme conversion modules. The quality of the speech synthesized by a Bengali TTS system after incorporation of the baseline text analysis block is evaluated by subjective listening tests.</li>
					<li>	Sentences are tokenized by the tokenization and splitting processes. Rule-based and decision tree based text normalization approaches are proposed for classifying and further disambiguating the non-standard words. Classified NSWs are converted to associated spoken forms using a look-up table and a set of hand-crafted rules.</li>
					<li>	Grapheme-to-phoneme conversion is achieved by basic manual and rule-based approaches. A manual dictionary is developed with 1,00,00 entries collected from various domains. Rule-based approach is proposed using orthographic rules for handling the out-of-vocabulary words. A subword method is proposed to reduce the manual dictionary size without compromising vocabulary coverage and to improve the performance of the rule-based approach. Memory-based data-driven approach is proposed to overcome the limitation of the rule-based approach.</li>
					<li>	Positional, structural and morphological features are proposed and their significances are analyzed in context of predicting the phrase breaks. With the help of model-dependent feature selection process, optimal feature sets are selected. CART and FFNN based phrase break prediction module is developed with the help of the proposed optimal features.</li>
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
    </div>
  </div>
  <hr>
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/avatar_mr.png" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Sudhamay Maity </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Language Identification for Indian Languages using Spectral and Prosodic features
	   <br><strong>Present Affiliation :</strong> Project Manager, Ozonetel Systems
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal8"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal8" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Sudhamay Maity</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	IITKGP-MLILSC (Indian Institute of Technology Kharagpur Multilingual Indian Language Speech Corpus) is developed for promoting LID research on ILs.</li>
					<li>	Spectral features from pitch synchronous analysis and glottal closure regions are proposed for discriminating the ILs.</li>
					<li>	Intonation, rhythm and stress features at individual syllable level and in the sequence of syllables within a word are proposed for identifying the ILs.</li>
					<li>	Prosodic patterns at global level are proposed in terms of variation of F 0 , intensity and duration patterns for recognizing the ILs.</li>
					<li>	Language-specific information from the spectral and prosodic features is combined for improving the accuracy of LID system. </li>
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
	   
    </div>
  </div>
  <hr>
 
   <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/dipanjan.JPG" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Dipanjan Nandi </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Language Identification using Excitation Source Features
	   <br><strong>Present Affiliation :</strong> PhD Scholar, University of Alberta (Canada)
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal9"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal9" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Dipanjan Nandi</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Methods are proposed to derive implicit features from LP residual, its magnitude and phase components for capturing language-specific information.</li>
					<li>	Methods are proposed for deriving explicit parametric features from LP residual signal for language discrimination task.</li>
					<li>	Combination of implicit and parametric features of excitation source has been explored to enhance the LID performance.</li>
					<li>	Combination of the evidences obtained from overall excitation source and vocal tract features has been explored to investigate the existence of complementary language-specific information present in these two features.</li>
					<li>	The robustness of excitation source information has been explored for language identification task in terms of varying (i) background noise, (ii) amount of training data and (iii) duration of test samples. </li>
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
	   
	   
    </div>
  </div>
  <hr>
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/sourjya.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Sourjya Sarkar</h4>
      <p><strong>Area of Research :</strong> Speaker Recognition
       <br><strong>Thesis Title :</strong> Robust Speaker Recognition in Noisy Environments
	   <br><strong>Present Affiliation :</strong> Graduate Research Assistant, Johns Hopkins University (US)
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal10"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal10" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Sourjya Sarkar</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Feature compensation using multiple background models has been proposed for SV in noisy background environments.</li>
					<li>	Integration of data-driven stochastic feature compensation methods in the GMM-UBM framework has been proposed for improved robust speaker verification (SV) in noisy background environments under mismatched conditions.</li>
					<li>	The robustness of GMM-SVM framework for speaker modeling have been explored for SV in noisy environments under matched conditions.</li>
					<li>	The robustness of total variability modeling (i-vectors) in a discriminative framework has been explored for SV in noisy environments.</li>
					<li>	Utterance partitioning has been proposed for enhancing SV performances in noisy environments. </li>
					<li>	A boosting algorithm has been proposed for combining robust SVM classifiers for improving SV performance </li>
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
    </div>
  </div>
  <hr>
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/sunil.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Sunil Kumar S B </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Analysis and synthesis of vocal fold activity using an electroglottographic signal
	   <br><strong>Present Affiliation : </strong> Software Engineer, Cisco
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal11"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal11" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Sunil Kumar S B</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Robust method is proposed to analyse the activity of vocal fold using the phase of an EGG signal.</li>
					<li>	Two significant instants within a glottal cycle, namely glottal opened and glottal closed instants are proposed, in addition to the existing glottal closure and glottal opening instants. Using the phase of an EGG signal, robust and accurate methods to detect glottal opened instant, glottal closed instant, glottal closure instant and glottal opening instant are proposed.</li>
					<li>	A method is proposed to synthesize the activity of vocal fold using significant instants within a glottal cycle and the phase of an EGG signal.</li>
					<li>	A precise method is proposed for automatic synchronization of speech and EGG signals, using glottal activity information present in speech and EGG signals.</li>
					<li>	A robust voice/non-voice detection method based on harmonics of phase of zero frequency filtered speech signal is proposed. </li>
					
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
    </div>
  </div>
  <hr>
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/manjunath.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Manjunath K E </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Articulatory and Excitation Source Features for Phone Recognition
	   <br><strong>Present Affiliation :</strong>  Scientist, ISRO 
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal12"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal12" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Manjunath K E</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Speech data in read, extempore and conversation modes of Bengali language is collected and manually transcribed using international phonetic alphabet chart.</li>
					<li>	Methods are proposed to derive the articulatory features from the spectral features using FFNNs.</li>
					<li>	The development of phone recognition systems using combination of spectral and articulatory features is proposed.</li>
					<li>	Methods are proposed to capture the excitation source information from the LP residual of the speech signal.</li>
					<li>	The development of phone recognition systems using combination of spectral and excitation source features is proposed. </li>
					<li>	The articulatory and excitation source features are analyzed across read, extempore and conversation modes of speech. </li>
					
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
    </div>
  </div>
  <hr>
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/gurunath.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Gurunath Reddy M </h4>
      <p><strong>Area of Research :</strong> Music Signal Processing
       <br><strong>Thesis Title :</strong> Predominant Melody Extraction from Vocal Polyphonic Music Signals
	   <br><strong>Present Affiliation :</strong> PhD Scholar, IIT Kharagpur 
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal13"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal13" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Gurunath Reddy M</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Dataset is developed for Hindustani polyphonic music (IITKGP HPMD).</li>
					<li>	A multi-stage spectro-temporal framework has been proposed for melody extraction from polyphonic music.</li>
					<li>	A novel percussion suppression method is developed to suppress the non-pitched percussive source in the polyphonic music signal.</li>
					<li>	The note onset detection method based on the normalized spectral energy change is proposed.</li>
					<li>	A novel frequency deviation weighted resonance frequency detection algorithm is proposed to design the mean subtraction filter for zero frequency filtering method. </li>
					<li>	The singer vocal tonic detection and Tanpura tuning system has been developed using the proposed melody extraction method. </li>
					<li>	A robust SARGAM evaluation tool has been developed for learners. </li>
					<li>	An application is developed for learners, to synthesize the correct SARGAM sequence from the out-of-tune note sequences </li>
					
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
    </div>
  </div>
  <hr>
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/hari.JPG" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Harikrishna D M </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Multi-stage Children Story Speech Synthesis
	   <br><strong>Present Affiliation :</strong> Associate Software Engineer, HARMAN International India Pvt. Ltd.  
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal14"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal14" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Harikrishna D M</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Developed text corpora for story classification, and speech database for expressive speech synthesis in Hindi and Telugu.</li>
					<li>	An end-to-end multi-stage framework for synthesizing story-style speech from story text is proposed.</li>
					<li>	A framework for classifying Hindi and Telugu children stories using keyword and POS based features is proposed.</li>
					<li>	A framework for classifying Hindi and Telugu children stories using partial story information is proposed.</li>
					<li>	Emotion-specific features are proposed to predict the sentence level emotion from Hindi story text. </li>
					<li>	An approach to updating the emotions based on the knowledge of the sequence of previous emotions present in the story text is proposed. </li>
					
					
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
    </div>
  </div>
  <hr>
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/parakrant.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Parakrant Sarkar </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Prosody Modeling for Storytelling Style Speech Synthesis
	   <br><strong>Present Affiliation :</strong> Speech Recognition Software Developer, Vocera
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal15"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal15" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Parakrant Sarkar</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	Development of Story TTS system using neutral TTS system.</li>
					<ul>
							<li>	We proposed three additional modules that can easily be plugged in with the neutral TTS to generate story style speech. Additionally developed modules are: (i) Story-specific emotion detection (SSED) module, (ii) Story-specific prosody generation (SSPG) module and (iii) Story-specific prosody incorporation (SSPI) module </li>
							<li>	The prosody rules are designed and derived by analyzing the differences between prosodic parameters of synthesized neutral style speech and original storyteller speech. The prosody rules are derived based on five different parameters namely, pitch, duration, intensity, tempo and pauses.</li>
					</ul>
					<li>	Development of Story TTS system using neutral TTS system.</li>
					<ul>
							<li>	Story-specific pause prediction model is used for handling two problems - <ul><li>Predicting the accurate position of the pause in an utterance</li> <li>Predicting the suitable duration for the pause </li> </li></ul>
							<li>	Developing story-specific pause prediction models with discourse information. The three modes of discourse considered are narrative, descriptive and dialogue.</li>
					</ul>
					<li>	Developing prosody models for duration, pitch and intensity based on story genre. We have considered three story genres namely, fable, folk-tale and legend.</li>
					<ul>
							<li>	The proposed three prosody models are developed using - <ul> <li>Positional contextual and phonetic (PCP) features</li> <li> Story-specific positional contextual and phonetic (PCP) and story-semantic features</li> <li>Story genre specific positional contextual and phonetic (PCP) and story-semantic features</li> </ul></li>
							
					</ul>
					
					
					
					
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
	   
	   
    </div>
  </div>
  <hr>
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/arijul.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Haque Arijul </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Emotion Conversion in Speech using Source, System and Prosody Modifications
	   <br><strong>Present Affiliation :</strong> PhD Scholar, IIT Kharagpur.  
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal16"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal16" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Haque Arijul</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	A method of modifying energy spectrum of neutral speech for emotion conversion based on filter banks is proposed. The filter bank divides the frequency spectrum into separate bands and the energy of these bands are modified separately.</li>
					<li>	A new method of modifying epoch strength is proposed, in which, we modified only those parts of the Hilbert envelope which contain epoch locations. The other parts are left unaffected.</li>
					<li>	A new method of modifying epoch sharpness is also proposed. Modification of epoch sharpness for emotion conversion.</li>
					<li>	We have also explored two GMM-based feature mapping techniques for emotion conversion. Line spectral frequencies (LSF) were used as features.</li>
					<li>	A side effect of the vocal tract transformation of neutral speech was that it sounded distorted after the transformation. The possible reasons behind these distortions have been investigated and a distortion reduction technique has been proposed to reduce the distortions that arise. </li>
					<li>	We have proposed a method to modify the intensity of neutral speech to generate emotional speech by segmenting the speech utterances into three equal parts and modifying their intensities separately based on some scaling factors. </li>
					<li>	A method of modifying the intensity contour based on polynomial regression has also been proposed. </li>
					
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
    </div>
  </div>
  <hr>
  
  
  <br>
  <div class="media" >
    <div class="media-left media-top">
      <img src="images/arup.jpg" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Arup Kumar Dutta </h4>
      <p><strong>Area of Research :</strong> Speech Processing
       <br><strong>Thesis Title :</strong> Robust Language Identification using Magnitude and Phase Information
	   <br><strong>Present Affiliation :</strong> Assistant Engineer (IT), DVC  
	   </p>
	   <div class="container">
		  <!-- Trigger the modal with a button -->
		  <button type="button" class="btn btn-default " data-toggle="modal" data-target="#myModal17"><b>Thesis Contribution</b></button>

		  <!-- Modal -->
		  <div class="modal fade" id="myModal17" role="dialog">
			<div class="modal-dialog modal-lg">
			
			  <!-- Modal content-->
			  <div class="modal-content">
				<div class="modal-header">
				  <button type="button" class="close" data-dismiss="modal">&times;</button>
				  <h4 class="modal-title">Thesis contribution of Arup Kumar Dutta</h4>
				</div>
				<div class="modal-body">
				 
				  <ol>
					<li>	The phase of the Fourier transform has been explored for language identification. In this regard, three phase-based features have been proposed for robust language identification. To capture the entire linguistic information present in speech, each phase-based system is combined with MFCC-based system and observed a notable improvement in recognition accuracy.</li>
					<li>	The effect of noise in the phase and magnitude based systems are analyzed. From the analysis, it is observed that the affect of noise is less in the phase-based system. However, both the systems suffer from the noise. Therefore, to make these systems robust, two GMM-based mapping techniques have been explored.</li>
					<li>	Power normalized cepstral coefficients (PNCCs) have been explored for robust language identification and observed that the system developed using PNCC features performs better than the MFCC-based system in the presence of noise.</li>
					
					
					
				</ol>

				 
				</div>
				<div class="modal-footer">
				  <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
				</div> <!--End Footer-->
			  </div><!-- End Modal content-->
			  
			</div>
		  </div>
		  
		</div>
	   
    </div>
  </div>
  <hr>
  
  
  
  
  <!--
  <div class="media">
    <div class="media-left media-middle">
      <img src="images/avatar_mr.png" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Media Middle</h4>
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
    </div>
  </div>
  <hr>
  <div class="media">
    <div class="media-left media-bottom">
      <img src="images/avatar_mr.png" class="media-object" style="width:80px">
    </div>
    <div class="media-body">
      <h4 class="media-heading">Media Bottom</h4>
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>
    </div>
  </div>
  
  !-->
  
  
  
</div>



          

           

          </div><!--/row-->
        </div><!--/span-->

       

      </div><!--/row-->

     <footer>
        <nav class="navbar navbar-default navbar-static-bottom" role="navigation">
        <p class="navbar-text">&copy Copyright 2018 Prof. K. Sreenivasa Rao</p>
        </nav>
      </footer>
        
    </div><!--/.container-->

    <!-- add javascripts -->
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
</body>
</html>
